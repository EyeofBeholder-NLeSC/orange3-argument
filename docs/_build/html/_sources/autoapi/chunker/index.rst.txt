:py:mod:`chunker`
=================

.. py:module:: chunker

.. autoapi-nested-parse::

   Argument chunker module



Module Contents
---------------

Classes
~~~~~~~

.. autoapisummary::

   chunker.TopicModel



Functions
~~~~~~~~~

.. autoapisummary::

   chunker.load_nlp_pipe
   chunker.get_chunk
   chunker.get_chunk_polarity_score
   chunker.get_chunk_topic
   chunker.get_chunk_rank
   chunker.get_chunk_table



.. py:function:: load_nlp_pipe(model_name: str)

   Download the required nlp pipe if not exist

   :param model_name: name of the nlp pipe, a full list of models can be found from https://spacy.io/usage/models.
   :type model_name: str

   :returns: The spacy nlp model.


.. py:function:: get_chunk(docs: List[str]) -> Tuple[List[int], List[str]]

   Split documents of a given corpus into chunks.

   A chunk can be considered as a meaningful clause, which can be part of a sentence. For instance, the sentence "I like the color of this car but it's too expensive." will be splitted as two chunks, which are "I like the color of this car" and "but it's too expensive". A dependency parser is implemented for doing this job.

   :param docs: The input corpus.
   :type docs: List[str]

   :returns: ids of the arguments that the chunks belongs to.
             List[str]: chunk text.
   :rtype: List[int]


.. py:function:: get_chunk_polarity_score(chunks: List[str])

   Compute polarity score of each chunk in the given list.

   The polarity score is a float within the range [-1.0, 1.0], where 0 means neutral, + means positive, and - means negative.

   :param chunks: chunk list
   :type chunks: List[str]

   :returns: polarity scores of the given chunks
   :rtype: List[float]


.. py:function:: get_chunk_topic(chunks: List[str])

   Get topic information and embedding vectors of chunks via topic modeling.

   :param chunks: chunk list.
   :type chunks: List[str]

   :returns: topic ids of chunks.
             np.ndarray: embedding vectors of chunks.
             pd.DataFrame: Table of topic information.
   :rtype: List[int]


.. py:function:: get_chunk_rank(arg_ids: List[int], embeds: numpy.ndarray)

   In each argument, comput rank of chunks within.

   Rank can be understood as importance of chunks. This function computes the relative importance of chunks within arguments they belong to. This is done by applying the Pagerank algorithm, where similarity is computed as the cosine similarity of chunk embedding vectors.

   :param arg_ids: ids of arguments that chunks belongs to.
   :type arg_ids: List[int]
   :param embeds: embedding vectors of chunks.
   :type embeds: np.ndarray

   :returns: rank of chunks
   :rtype: List[float]


.. py:function:: get_chunk_table(arg_ids: List[int], chunks: List[str], p_scores: List[float], topics: List[int], ranks: List[float])

   Given all the measures of chunks, generate and return the chunk table as a pandas dataframe, with pre-defined column names.

   :param arg_ids: ids of arguments that chunks belong to
   :type arg_ids: List[int]
   :param chunks: chunk text
   :type chunks: List[str]
   :param p_scores: polarity score of chunks
   :type p_scores: List[float]
   :param topics: topic id of chunks
   :type topics: List[int]
   :param ranks: rank of chunks
   :type ranks: List[float]

   :returns: chunk table
   :rtype: pd.DataFrame


.. py:class:: TopicModel


   Topic modeling class.

   Functions are implemented based on the BERTopic model. For now, the topic model is setup with a set of default parameters of the sub-models. However, it should be possible that the user can config it further. This will be a next step.

   .. attribute:: _rd_model (

      obj:'UMAP'): instance of UMAP algorithm as the dimensionality reduction sub-model.

   .. attribute:: model (

      obj:'BERTopic'): the topic model that applied the sub-models predefined.

   .. py:method:: init_model(transformer: str = 'all-mpnet-base-v1', n_components: int = 5, min_cluster_size: int = 10, ngram_min: int = 1, ngram_max: int = 1)

      Initialize the topic model by indicating a number of arguments.

      :param transformer: Name of the sentence embedding model. Defaults to "all-mpnet-base-v1". A list of pretrained models can be found here: https://www.sbert.net/docs/pretrained_models.html.
      :type transformer: str, optional
      :param n_components: Number of dimensions after reduction. Defaults to 5.
      :type n_components: int, optional
      :param min_cluster_size: Minimum size of clusters for the clustering algorithm. Defaults to 5.
      :type min_cluster_size: int, optional
      :param ngram_min: Low band of ngram range for topic representation. Defaults to 1.
      :type ngram_min: int, optional
      :param ngram_max: High band of ngram range for topic representation. Defaults to 1.
      :type ngram_max: int, optional


   .. py:method:: fit_transform_reduced(docs: List[str]) -> List[int]

      Further reduce outliers from the result of the fit_transform function.

      Note that BERTopic is a clustering approach, which means that it doesn not work if there is nothing to be clustered. And keep in mind that the input corpus should contain at least 1000 documents to get meaningful results. Refer to this thread: https://github.com/MaartenGr/BERTopic/issues/59#issuecomment-775718747.

      :param docs: The input corpus.
      :type docs: List[str]

      :returns: Topics of the input docs.
      :rtype: List[int]


   .. py:method:: get_topic_table() -> pandas.DataFrame

      Get the table of topic information and return it as a pandas dataframe.

      :returns: The topic table.
      :rtype: pd.DataFrame


   .. py:method:: get_doc_embeds() -> numpy.ndarray

      Get the embeddings of the docs.

      :returns: Embeddings of the docs, in size of (n_doc, n_components).
      :rtype: np.ndarray



